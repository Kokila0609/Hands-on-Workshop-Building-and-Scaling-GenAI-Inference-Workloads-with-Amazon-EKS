Introduction

In this section, we'll explore and validate our workshop environment, ensuring everything is properly configured for running generative AI workloads on Amazon EKS. Amazon EKS serves as our foundation for hosting and deploying Large Language Models (LLMs), providing a robust and scalable infrastructure for these resource-intensive workloads.

What We'll Explore
Workshop Infrastructure: Understanding the pre-provisioned environment including VPC and EKS cluster
Base Cluster Configuration: Examining our initial node setup with non-GPU instances
Storage Configuration: Exploring the FSx CSI Driver setup for high-performance storage
Core Components: Overview of essential cluster add-ons and their roles, NVIDIA Device Plugin and GPU access
